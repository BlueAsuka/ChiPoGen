{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.memmap('../data/train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap('../data/val.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Randomly select chunk of text for training\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+block_size].astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+1+block_size].astype(np.int64)) for i in ix])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12276,  1375,    44,     0,  8401,  3652, 12471,  2911,  3268,  1268,\n",
       "         12953,  4905,  8635,  7521, 11435,  3300,  4899,    44,     0,     0,\n",
       "          5400,  5758,  1776,  7373,  4551, 12953,  1336,  2683,  4913,  1361,\n",
       "          4971,    44],\n",
       "        [ 6102,  5240,  2156,  5921,    44,     0,     0,  5707, 11982,  5691,\n",
       "          7826, 12605, 12953,  1387,  3696,  3003,  2055,  2172,    44,     0,\n",
       "          4826,  2605,  1296,  5984, 12772, 12953, 10863,  4417,  6103,  5773,\n",
       "          4839,    44],\n",
       "        [ 3871,  5024,    36,    36,  5449, 12953,  9464,  2651,  7386,  9098,\n",
       "          2250,  3311, 10857,    44,     0,  4858,  4178,  1285, 12841, 10912,\n",
       "          7195,  2113, 12953,  4930,  1390,  1342,  1414,  2051, 11079,  3344,\n",
       "            44,     0],\n",
       "        [ 9507, 11658,  4899,    44,     0,  3723,  7735,  3477,  1286,  7737,\n",
       "          1779,  3309, 12953,  2603,  4867,  1369,  9218,  7733,  3810,  4784,\n",
       "            44,     0,  2643, 10359,  4148,  5234,  7762,  2956,  2644, 12953,\n",
       "          2623,  7758],\n",
       "        [ 1375,  4093,    44,     0,  1767,  1850,  1570,  1310,  1299,  4269,\n",
       "          7106, 12953,  1267,  3963,  2912, 11162,  8793,  1267,  1621,    44,\n",
       "             0,     0,  3695,  7094,  9957,  5629,  2145,  7105,  2901, 12953,\n",
       "          4782,  4362],\n",
       "        [11245, 10595,    44,     0,  3312,  3696,  4347, 11206,  7399, 10478,\n",
       "          9048,    44,     0,  9098,  2891,  4899, 12016,  5090,  5190,  2909,\n",
       "            44,     0, 10970,  1395,  5653,  3380,  2122,  7925,  6431, 12953,\n",
       "          4108,  5025],\n",
       "        [ 6470, 12953,  5555,  7154,  5024,  2039,  6879,    44,     0, 12078,\n",
       "         10420,  6087,  1825, 10623, 12953,  5957,  5047, 10166,  2901,  9047,\n",
       "            44,     0,     0,  9211,  5326,  1280,  1291,  3312, 11041, 11487,\n",
       "         12953,  3835],\n",
       "        [10706,  9639,  2183,  2873,  3960, 10034,  7404,    44,     0,  2234,\n",
       "          2181,  3882,  1311,  9048,  6531,  2445, 12953,  7754,  6373, 11978,\n",
       "          2249,  1348, 12113,  5745,    44,     0,  6551,  7420,  6475,  2956,\n",
       "          9666,  7108],\n",
       "        [   36,    36,  2956,  4821,  1303,  1257, 11706, 12953, 10398, 11832,\n",
       "         11572,  2896, 11578, 11250,  1312,    44,     0, 10068,  3332, 12193,\n",
       "         11839,  7399,    36,    36, 12953,    36,    36,    36,  3370,  5946,\n",
       "          1262,  2903],\n",
       "        [10909,  1987,  4851,  1542,  4894, 12953,  8081,  4888,  2644,  7298,\n",
       "          4913,    44,     0,  2171,  1955,  1407,  1299,  1829, 12953,  1257,\n",
       "          1817,  1267,  2150,  2122,    44,     0,     0,  6679,  1375,  1316,\n",
       "          1265,  6679],\n",
       "        [ 1375,    44,     0,  1267,  4905,  2994, 12083,  8985, 12953,  4164,\n",
       "         10202,  2181,  6647, 10793,    44,     0,  3243,  5293,  3140, 12603,\n",
       "          4858, 12953,  3835,  3905,  4020, 12193, 10948,    44,     0,  4148,\n",
       "          2181,  7126],\n",
       "        [    0,     0,  6067,  2901,  2956,  5745, 11774,  2956,  3315, 12953,\n",
       "          5653, 11761,  2262, 11796,  1821,  7061, 12486,    44,     0,  8438,\n",
       "          2946,  4851, 10202,  2034,  2140,  4020, 12953,  8285,  3057,  5025,\n",
       "          4139,  2033],\n",
       "        [ 9649,    44,     0,     0,  1480,  1341,  4884,  4867,  5876,  6852,\n",
       "         11978, 12953, 11572,  3225,  1267, 10159,  5071,  2613,  4899,    44,\n",
       "             0,  1848,  7306, 10348,  1265,  9669,  5847,  2250, 12953, 12667,\n",
       "         12640,  4320],\n",
       "        [ 3302,  4894,    44,     0,  3370,  2644,  9649,  4851,  8528,  8500,\n",
       "         11605, 12953,  7373,  5038,  2171,  4905,  1257,  1329,  1774,    44,\n",
       "             0,  5602,  3309,  3899, 12825,  4276, 12276,  1438, 12953,  1274,\n",
       "         11028,  5519],\n",
       "        [ 6852, 11811,  6255, 12953,  2172,  9098,  4717,  1857,  1348,  1346,\n",
       "          2609,    44,     0,  1299,  3300,  6679,  6699,  4600,  6963,  2880,\n",
       "         12953,  1757,  3882,  1263, 11090,  3652,  8008,  7386,    44,     0,\n",
       "             0,  7151],\n",
       "        [10415, 12953,  1299,  3206,  1311,  1766, 11593,    44,     0,  1307,\n",
       "          7756,  6611,  2173,  1375, 12953,  5025,  1330,  1546,  1267, 10531,\n",
       "            44,     0,  8834,  1406,  1342,  1754,  7094, 12953,  8775,  5088,\n",
       "          1773,  4821]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1375,    44,     0,  8401,  3652, 12471,  2911,  3268,  1268, 12953,\n",
       "          4905,  8635,  7521, 11435,  3300,  4899,    44,     0,     0,  5400,\n",
       "          5758,  1776,  7373,  4551, 12953,  1336,  2683,  4913,  1361,  4971,\n",
       "            44,     0],\n",
       "        [ 5240,  2156,  5921,    44,     0,     0,  5707, 11982,  5691,  7826,\n",
       "         12605, 12953,  1387,  3696,  3003,  2055,  2172,    44,     0,  4826,\n",
       "          2605,  1296,  5984, 12772, 12953, 10863,  4417,  6103,  5773,  4839,\n",
       "            44,     0],\n",
       "        [ 5024,    36,    36,  5449, 12953,  9464,  2651,  7386,  9098,  2250,\n",
       "          3311, 10857,    44,     0,  4858,  4178,  1285, 12841, 10912,  7195,\n",
       "          2113, 12953,  4930,  1390,  1342,  1414,  2051, 11079,  3344,    44,\n",
       "             0, 11668],\n",
       "        [11658,  4899,    44,     0,  3723,  7735,  3477,  1286,  7737,  1779,\n",
       "          3309, 12953,  2603,  4867,  1369,  9218,  7733,  3810,  4784,    44,\n",
       "             0,  2643, 10359,  4148,  5234,  7762,  2956,  2644, 12953,  2623,\n",
       "          7758,  8481],\n",
       "        [ 4093,    44,     0,  1767,  1850,  1570,  1310,  1299,  4269,  7106,\n",
       "         12953,  1267,  3963,  2912, 11162,  8793,  1267,  1621,    44,     0,\n",
       "             0,  3695,  7094,  9957,  5629,  2145,  7105,  2901, 12953,  4782,\n",
       "          4362,  2911],\n",
       "        [10595,    44,     0,  3312,  3696,  4347, 11206,  7399, 10478,  9048,\n",
       "            44,     0,  9098,  2891,  4899, 12016,  5090,  5190,  2909,    44,\n",
       "             0, 10970,  1395,  5653,  3380,  2122,  7925,  6431, 12953,  4108,\n",
       "          5025, 10356],\n",
       "        [12953,  5555,  7154,  5024,  2039,  6879,    44,     0, 12078, 10420,\n",
       "          6087,  1825, 10623, 12953,  5957,  5047, 10166,  2901,  9047,    44,\n",
       "             0,     0,  9211,  5326,  1280,  1291,  3312, 11041, 11487, 12953,\n",
       "          3835,  5088],\n",
       "        [ 9639,  2183,  2873,  3960, 10034,  7404,    44,     0,  2234,  2181,\n",
       "          3882,  1311,  9048,  6531,  2445, 12953,  7754,  6373, 11978,  2249,\n",
       "          1348, 12113,  5745,    44,     0,  6551,  7420,  6475,  2956,  9666,\n",
       "          7108, 10602],\n",
       "        [   36,  2956,  4821,  1303,  1257, 11706, 12953, 10398, 11832, 11572,\n",
       "          2896, 11578, 11250,  1312,    44,     0, 10068,  3332, 12193, 11839,\n",
       "          7399,    36,    36, 12953,    36,    36,    36,  3370,  5946,  1262,\n",
       "          2903,    44],\n",
       "        [ 1987,  4851,  1542,  4894, 12953,  8081,  4888,  2644,  7298,  4913,\n",
       "            44,     0,  2171,  1955,  1407,  1299,  1829, 12953,  1257,  1817,\n",
       "          1267,  2150,  2122,    44,     0,     0,  6679,  1375,  1316,  1265,\n",
       "          6679, 12953],\n",
       "        [   44,     0,  1267,  4905,  2994, 12083,  8985, 12953,  4164, 10202,\n",
       "          2181,  6647, 10793,    44,     0,  3243,  5293,  3140, 12603,  4858,\n",
       "         12953,  3835,  3905,  4020, 12193, 10948,    44,     0,  4148,  2181,\n",
       "          7126,  2039],\n",
       "        [    0,  6067,  2901,  2956,  5745, 11774,  2956,  3315, 12953,  5653,\n",
       "         11761,  2262, 11796,  1821,  7061, 12486,    44,     0,  8438,  2946,\n",
       "          4851, 10202,  2034,  2140,  4020, 12953,  8285,  3057,  5025,  4139,\n",
       "          2033,  1867],\n",
       "        [   44,     0,     0,  1480,  1341,  4884,  4867,  5876,  6852, 11978,\n",
       "         12953, 11572,  3225,  1267, 10159,  5071,  2613,  4899,    44,     0,\n",
       "          1848,  7306, 10348,  1265,  9669,  5847,  2250, 12953, 12667, 12640,\n",
       "          4320, 12276],\n",
       "        [ 4894,    44,     0,  3370,  2644,  9649,  4851,  8528,  8500, 11605,\n",
       "         12953,  7373,  5038,  2171,  4905,  1257,  1329,  1774,    44,     0,\n",
       "          5602,  3309,  3899, 12825,  4276, 12276,  1438, 12953,  1274, 11028,\n",
       "          5519,  8556],\n",
       "        [11811,  6255, 12953,  2172,  9098,  4717,  1857,  1348,  1346,  2609,\n",
       "            44,     0,  1299,  3300,  6679,  6699,  4600,  6963,  2880, 12953,\n",
       "          1757,  3882,  1263, 11090,  3652,  8008,  7386,    44,     0,     0,\n",
       "          7151,  7151],\n",
       "        [12953,  1299,  3206,  1311,  1766, 11593,    44,     0,  1307,  7756,\n",
       "          6611,  2173,  1375, 12953,  5025,  1330,  1546,  1267, 10531,    44,\n",
       "             0,  8834,  1406,  1342,  1754,  7094, 12953,  8775,  5088,  1773,\n",
       "          4821,  4798]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "bias = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 1152])\n",
      "Split the last dimension into q, k, v\n",
      "Shape of q, k, v: torch.Size([16, 32, 384]), torch.Size([16, 32, 384]), torch.Size([16, 32, 384])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(batch_size, block_size, n_embd)\n",
    "attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "print(attn(x1).shape)\n",
    "print(f'Split the last dimension into q, k, v')\n",
    "q, k, v = torch.split(attn(x1), split_size_or_sections=n_embd, dim=2)\n",
    "print(f'Shape of q, k, v: {q.shape}, {k.shape}, {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of q, k, v: torch.Size([16, 6, 32, 64]), torch.Size([16, 6, 32, 64]), torch.Size([16, 6, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "# calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "q = q.view(batch_size, block_size, n_head, n_embd // n_head).transpose(1, 2)\n",
    "k = k.view(batch_size, block_size, n_head, n_embd // n_head).transpose(1, 2)\n",
    "v = v.view(batch_size, block_size, n_head, n_embd // n_head).transpose(1, 2)\n",
    "print(f'Shape of q, k, v: {q.shape}, {k.shape}, {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "mask = torch.tril(torch.ones(block_size, block_size))\n",
    "mask = mask.view(1, 1, block_size, block_size) # Add the batch dimension\n",
    "mask[:, :, :block_size, :block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_proj = nn.Linear(n_embd, n_embd, bias)\n",
    "attn_dropout = nn.Dropout(dropout)\n",
    "resid_dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "att = att.masked_fill(mask=(mask[:, :, :block_size, :block_size] == 0), value=float('-inf'))\n",
    "att = F.softmax(att, dim=-1)\n",
    "att = attn_dropout(att)\n",
    "y1 = att @ v # (B, nh, T, T) x (B, nh, T, hs) => (B, nh, T, hs)\n",
    "# (B, nh, T, hs) => (B, T, nh, hs) => (B, T, nh * hs) => (B, T, C)\n",
    "y1 = y1.transpose(1, 2).contiguous().view(batch_size, block_size, n_embd)\n",
    "y1 = c_proj(y1)\n",
    "y1 = resid_dropout(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 384])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"The remainder of embedding and head number should be zero.\"\n",
    "        \n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer('bias', torch.tril(torch.ones(block_size, block_size))\n",
    "                                          .view(1, 1, block_size, block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch_size, sequence length (block_size), embedding dimension (n_embd)\n",
    "        \n",
    "        q, k, v = torch.split(self.c_attn(x), split_size_or_sections=n_embd, dim=2)\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(mask=(self.bias[:, :, :block_size, :block_size] == 0), value=float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        return resid_dropout(c_proj(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(n_dim))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Bias is not used in this model\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, bias=None, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(n_embd, 4 * n_embd, bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_embd)\n",
    "        self.attn = SelfAttention()\n",
    "        self.ln_2 = LayerNorm(n_embd)\n",
    "        self.mlp = MLP()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 32\n",
    "    vocab_size: int = 12992\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 384 \n",
    "    dropout: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12992, 384]), torch.Size([12992, 384])\n"
     ]
    }
   ],
   "source": [
    "emb_w = nn.Embedding(config.vocab_size, config.n_embd).weight\n",
    "proj_w = nn.Linear(config.n_embd, config.vocab_size, bias=False).weight\n",
    "print(f'{emb_w.shape}, {proj_w.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "drop = nn.Dropout(config.dropout)\n",
    "self_attn = nn.ModuleList([SelfAttentionBlock() for _ in range(config.n_layer)])\n",
    "ln_f = LayerNorm(config.n_embd)\n",
    "\n",
    "lm_head = nn.Linear(config.n_embd, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, t = x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = wte(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.arange(0, t, dtype=torch.long)\n",
    "pos_emb = wpe(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = drop(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in self_attn:\n",
    "    h = block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ln_f(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0616, -2.8554,  2.5387,  ...,  0.5750, -0.2359,  0.7878],\n",
       "         [-0.7315,  0.4567,  0.1484,  ...,  2.6605, -0.6875,  0.2774],\n",
       "         [-1.3246, -0.3199, -0.0086,  ..., -0.1334, -1.1427,  0.9465],\n",
       "         ...,\n",
       "         [ 0.4935,  0.9426,  0.7683,  ..., -2.3904, -0.5727, -0.4588],\n",
       "         [ 0.9080, -0.1385,  1.3717,  ...,  0.6032, -2.4350, -1.1443],\n",
       "         [-0.2722,  0.1268, -0.4437,  ...,  0.1638, -1.0512,  1.0896]],\n",
       "\n",
       "        [[-0.2998,  0.1574,  1.8886,  ...,  0.4090,  1.0215,  1.2194],\n",
       "         [-0.0888,  1.5727,  1.7173,  ...,  0.3340, -0.3872,  0.4777],\n",
       "         [-1.7268, -0.3312,  1.3689,  ..., -2.2727, -0.9690,  1.4416],\n",
       "         ...,\n",
       "         [-0.7515,  1.2760, -0.7825,  ..., -0.7137,  0.4080, -0.6182],\n",
       "         [-0.5858, -0.0110,  0.2162,  ..., -0.4953, -1.1886, -0.5534],\n",
       "         [-0.1956,  0.2227, -0.4753,  ...,  0.0632, -1.1565,  1.3665]],\n",
       "\n",
       "        [[ 0.8393,  1.0732,  0.5114,  ..., -0.0429,  0.7551,  1.5836],\n",
       "         [-0.3344,  1.0009,  0.2859,  ...,  0.6185, -0.2692, -0.0731],\n",
       "         [-1.0560,  0.0414, -0.5047,  ..., -0.2834, -0.8614,  2.0373],\n",
       "         ...,\n",
       "         [ 0.3190,  0.4915, -0.3811,  ..., -0.8594, -0.5453, -0.3678],\n",
       "         [ 0.2407,  0.3349,  1.1751,  ...,  0.7538, -1.5530,  0.2654],\n",
       "         [ 0.0043, -0.1592, -0.3821,  ...,  0.4349, -0.3163,  0.8983]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0861,  1.2173,  1.4394,  ...,  0.8131,  1.4689,  1.5101],\n",
       "         [-0.6314,  1.7089,  0.7403,  ...,  0.5251,  0.5008, -1.8246],\n",
       "         [-0.9393,  0.5727, -0.3259,  ..., -0.2320, -0.4813,  1.0745],\n",
       "         ...,\n",
       "         [-0.6386,  0.8968,  0.0481,  ..., -0.6851,  0.2055, -0.9721],\n",
       "         [ 0.7084,  0.1266,  1.7621,  ..., -0.7020, -0.6997, -1.4356],\n",
       "         [-1.4234,  0.7195,  0.7914,  ..., -1.0629, -0.3308,  1.3475]],\n",
       "\n",
       "        [[-1.8192,  0.6494,  2.7981,  ...,  1.5196,  0.7997,  1.4967],\n",
       "         [-0.8097,  0.4204,  0.8681,  ...,  1.1528,  0.3438,  0.5819],\n",
       "         [-0.4684,  1.2563,  1.2160,  ...,  0.1874, -0.8416,  1.3628],\n",
       "         ...,\n",
       "         [ 0.0324,  0.8264, -0.3093,  ..., -0.8477,  0.8588, -1.1576],\n",
       "         [ 0.2044,  0.5626,  1.5609,  ...,  0.4786, -0.3770, -0.6532],\n",
       "         [-0.5375,  0.0943, -0.3223,  ..., -0.4098, -0.0689,  2.2724]],\n",
       "\n",
       "        [[ 1.1589, -1.0181,  0.8261,  ...,  1.1488,  0.7363,  1.8116],\n",
       "         [-1.7253,  2.4894,  0.1752,  ...,  1.6741, -0.1542, -0.4371],\n",
       "         [-1.4826, -0.6076, -0.1095,  ..., -1.3109, -0.7236,  0.6495],\n",
       "         ...,\n",
       "         [ 1.4129, -0.2146, -0.1834,  ...,  0.2692, -0.6682, -0.8506],\n",
       "         [-0.0458,  0.5179,  0.4392,  ...,  1.6387, -0.2196, -0.0107],\n",
       "         [-0.6265, -0.0331, -0.3697,  ..., -0.4303,  0.3774,  1.3459]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 384])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 384])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_step = res[:, [-1], :] # note: using list [-1] to preserve the time dim\n",
    "last_step.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 12992])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0269, -0.3299,  0.1040,  ..., -0.6036,  0.8881,  0.5105]],\n",
       "\n",
       "        [[ 0.1028, -0.4665,  0.0260,  ..., -0.4697,  0.8926,  0.4367]],\n",
       "\n",
       "        [[ 0.0830,  0.2496,  0.0176,  ..., -0.0999,  0.8963,  0.9351]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2034,  0.1112,  0.1992,  ...,  0.2128,  0.2024,  1.2172]],\n",
       "\n",
       "        [[ 0.7887,  0.4334,  0.2768,  ...,  0.1259,  0.1355,  0.3733]],\n",
       "\n",
       "        [[-0.3308,  0.3745,  0.9104,  ..., -0.5784,  1.0488,  0.4708]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_logits = lm_head(last_step)\n",
    "print(inference_logits.shape)\n",
    "inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12992])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0269, -0.3299,  0.1040,  ..., -0.6036,  0.8881,  0.5105],\n",
       "        [ 0.1028, -0.4665,  0.0260,  ..., -0.4697,  0.8926,  0.4367],\n",
       "        [ 0.0830,  0.2496,  0.0176,  ..., -0.0999,  0.8963,  0.9351],\n",
       "        ...,\n",
       "        [ 0.2034,  0.1112,  0.1992,  ...,  0.2128,  0.2024,  1.2172],\n",
       "        [ 0.7887,  0.4334,  0.2768,  ...,  0.1259,  0.1355,  0.3733],\n",
       "        [-0.3308,  0.3745,  0.9104,  ..., -0.5784,  1.0488,  0.4708]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inference_logits[:, -1, :].shape)\n",
    "inference_logits = inference_logits[:, -1, :]\n",
    "inference_logits / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2205],\n",
       "        [ 6291],\n",
       "        [ 7252],\n",
       "        [ 9763],\n",
       "        [12030],\n",
       "        [  850],\n",
       "        [12805],\n",
       "        [10389],\n",
       "        [  173],\n",
       "        [ 6519],\n",
       "        [ 3376],\n",
       "        [ 9164],\n",
       "        [ 5579],\n",
       "        [ 5365],\n",
       "        [12385],\n",
       "        [ 1417]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(inference_logits, dim=-1)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm_head(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 12992])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0369,  0.1713,  0.1788,  ..., -0.2416,  0.2519, -0.0380],\n",
       "        [-0.4233, -0.3859,  0.4675,  ..., -0.4487,  0.6512,  0.4881],\n",
       "        [-0.4201, -0.2159,  0.0110,  ..., -0.4234,  0.6843, -0.4219],\n",
       "        ...,\n",
       "        [ 0.3948,  0.2101,  1.1524,  ..., -1.0070, -0.4542,  0.9740],\n",
       "        [ 0.5829,  0.8687,  0.0483,  ...,  1.1608,  0.0434,  0.1346],\n",
       "        [-0.3308,  0.3745,  0.9104,  ..., -0.5784,  1.0488,  0.4708]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits.view(-1, logits.size(-1)) # (B, T, C) => (B * T, C)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1375,    44,     0,  8401,  3652, 12471,  2911,  3268,  1268, 12953,\n",
       "          4905,  8635,  7521, 11435,  3300,  4899,    44,     0,     0,  5400,\n",
       "          5758,  1776,  7373,  4551, 12953,  1336,  2683,  4913,  1361,  4971,\n",
       "            44,     0],\n",
       "        [ 5240,  2156,  5921,    44,     0,     0,  5707, 11982,  5691,  7826,\n",
       "         12605, 12953,  1387,  3696,  3003,  2055,  2172,    44,     0,  4826,\n",
       "          2605,  1296,  5984, 12772, 12953, 10863,  4417,  6103,  5773,  4839,\n",
       "            44,     0],\n",
       "        [ 5024,    36,    36,  5449, 12953,  9464,  2651,  7386,  9098,  2250,\n",
       "          3311, 10857,    44,     0,  4858,  4178,  1285, 12841, 10912,  7195,\n",
       "          2113, 12953,  4930,  1390,  1342,  1414,  2051, 11079,  3344,    44,\n",
       "             0, 11668],\n",
       "        [11658,  4899,    44,     0,  3723,  7735,  3477,  1286,  7737,  1779,\n",
       "          3309, 12953,  2603,  4867,  1369,  9218,  7733,  3810,  4784,    44,\n",
       "             0,  2643, 10359,  4148,  5234,  7762,  2956,  2644, 12953,  2623,\n",
       "          7758,  8481],\n",
       "        [ 4093,    44,     0,  1767,  1850,  1570,  1310,  1299,  4269,  7106,\n",
       "         12953,  1267,  3963,  2912, 11162,  8793,  1267,  1621,    44,     0,\n",
       "             0,  3695,  7094,  9957,  5629,  2145,  7105,  2901, 12953,  4782,\n",
       "          4362,  2911],\n",
       "        [10595,    44,     0,  3312,  3696,  4347, 11206,  7399, 10478,  9048,\n",
       "            44,     0,  9098,  2891,  4899, 12016,  5090,  5190,  2909,    44,\n",
       "             0, 10970,  1395,  5653,  3380,  2122,  7925,  6431, 12953,  4108,\n",
       "          5025, 10356],\n",
       "        [12953,  5555,  7154,  5024,  2039,  6879,    44,     0, 12078, 10420,\n",
       "          6087,  1825, 10623, 12953,  5957,  5047, 10166,  2901,  9047,    44,\n",
       "             0,     0,  9211,  5326,  1280,  1291,  3312, 11041, 11487, 12953,\n",
       "          3835,  5088],\n",
       "        [ 9639,  2183,  2873,  3960, 10034,  7404,    44,     0,  2234,  2181,\n",
       "          3882,  1311,  9048,  6531,  2445, 12953,  7754,  6373, 11978,  2249,\n",
       "          1348, 12113,  5745,    44,     0,  6551,  7420,  6475,  2956,  9666,\n",
       "          7108, 10602],\n",
       "        [   36,  2956,  4821,  1303,  1257, 11706, 12953, 10398, 11832, 11572,\n",
       "          2896, 11578, 11250,  1312,    44,     0, 10068,  3332, 12193, 11839,\n",
       "          7399,    36,    36, 12953,    36,    36,    36,  3370,  5946,  1262,\n",
       "          2903,    44],\n",
       "        [ 1987,  4851,  1542,  4894, 12953,  8081,  4888,  2644,  7298,  4913,\n",
       "            44,     0,  2171,  1955,  1407,  1299,  1829, 12953,  1257,  1817,\n",
       "          1267,  2150,  2122,    44,     0,     0,  6679,  1375,  1316,  1265,\n",
       "          6679, 12953],\n",
       "        [   44,     0,  1267,  4905,  2994, 12083,  8985, 12953,  4164, 10202,\n",
       "          2181,  6647, 10793,    44,     0,  3243,  5293,  3140, 12603,  4858,\n",
       "         12953,  3835,  3905,  4020, 12193, 10948,    44,     0,  4148,  2181,\n",
       "          7126,  2039],\n",
       "        [    0,  6067,  2901,  2956,  5745, 11774,  2956,  3315, 12953,  5653,\n",
       "         11761,  2262, 11796,  1821,  7061, 12486,    44,     0,  8438,  2946,\n",
       "          4851, 10202,  2034,  2140,  4020, 12953,  8285,  3057,  5025,  4139,\n",
       "          2033,  1867],\n",
       "        [   44,     0,     0,  1480,  1341,  4884,  4867,  5876,  6852, 11978,\n",
       "         12953, 11572,  3225,  1267, 10159,  5071,  2613,  4899,    44,     0,\n",
       "          1848,  7306, 10348,  1265,  9669,  5847,  2250, 12953, 12667, 12640,\n",
       "          4320, 12276],\n",
       "        [ 4894,    44,     0,  3370,  2644,  9649,  4851,  8528,  8500, 11605,\n",
       "         12953,  7373,  5038,  2171,  4905,  1257,  1329,  1774,    44,     0,\n",
       "          5602,  3309,  3899, 12825,  4276, 12276,  1438, 12953,  1274, 11028,\n",
       "          5519,  8556],\n",
       "        [11811,  6255, 12953,  2172,  9098,  4717,  1857,  1348,  1346,  2609,\n",
       "            44,     0,  1299,  3300,  6679,  6699,  4600,  6963,  2880, 12953,\n",
       "          1757,  3882,  1263, 11090,  3652,  8008,  7386,    44,     0,     0,\n",
       "          7151,  7151],\n",
       "        [12953,  1299,  3206,  1311,  1766, 11593,    44,     0,  1307,  7756,\n",
       "          6611,  2173,  1375, 12953,  5025,  1330,  1546,  1267, 10531,    44,\n",
       "             0,  8834,  1406,  1342,  1754,  7094, 12953,  8775,  5088,  1773,\n",
       "          4821,  4798]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1).shape # (B, T, 1) => (B * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7090, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, y.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = nn.ModuleDict(dict(\n",
    "    wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "    wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "    drop = nn.Dropout(config.dropout),\n",
    "    self_attn_block = nn.ModuleList([SelfAttentionBlock() for _ in range(config.n_layer)]),\n",
    "    ln_f = LayerNorm(config.n_embd)\n",
    "))\n",
    "\n",
    "lm_head = nn.Linear(config.n_embd, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = transformer.wte(x)\n",
    "device = x.device\n",
    "b, t = x.size()\n",
    "pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "pos_emb = transformer.wpe(pos)\n",
    "h = transformer.drop(tok_emb + pos_emb)\n",
    "for block in transformer.self_attn_block:\n",
    "    h = block(h)\n",
    "res = transformer.ln_f(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7024, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the loss\n",
    "logits = lm_head(res)\n",
    "F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 12992])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for generation in inference\n",
    "logits = lm_head(res[:, [-1], :])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
