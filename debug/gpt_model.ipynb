{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.memmap('../data/train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap('../data/val.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Randomly select chunk of text for training\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+block_size].astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+1+block_size].astype(np.int64)) for i in ix])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9437,  9393,    44,     0,  1666,  8986,  4756, 12410,  8558, 12953,\n",
       "         11718,  6188,  5512, 11399,  5465,    44,     0,  6445,  6487,  5047,\n",
       "          8135,  6428, 12953,  4822,  5024,  1257, 11416,  4808,    44,     0,\n",
       "             0,  6087],\n",
       "        [ 1473,    44,     0,  4620, 12110,  6728,  5619,  2208,  1267,  2087,\n",
       "         12953,  9283,  4093, 12016, 11759,  2966,  4822, 11984,    44,     0,\n",
       "          2033,  1342, 12807,  3303,  8966, 11083,  6531, 12953,  9233,  8829,\n",
       "          7826,  9048],\n",
       "        [11205, 11165,  5736,  7373, 10988,    44,     0,  8985,  2911, 11250,\n",
       "          6032,  1845,  1885,  6223, 12953,  7420,  3719, 12718,  1346, 11703,\n",
       "          5602,  7925,    44,     0, 12113,  1267,  4819, 12953,  2241, 11735,\n",
       "          7922,    44],\n",
       "        [ 9723,  2087,  9903,  9752, 12953,  1276,  1264,  9214,  5045,  3783,\n",
       "          3368,  9418,    44,     0, 10595,  3960,  4889, 11612,  5663,  3645,\n",
       "          1952, 12953,  2968,  7768,  1387, 10159,  6489,  2901,  3607,    44,\n",
       "             0,  4851],\n",
       "        [ 1960, 12953,  1375,  1375,  4412,  4318,  2146,  2921,  8459,    44,\n",
       "             0,  4347,  5244,  5602, 10602, 10602,  1267,  3882, 12953,  4564,\n",
       "          8011,  6602,  6487, 11572,  2442,  4026,    44,     0,  8422,  8422,\n",
       "          1831, 11811],\n",
       "        [ 3882,  3663,  8598,  5085, 12953,  1770,  2033,  3696,  5088,  2122,\n",
       "          1342,  3696,    44,     0,  9010,  9025,  1299,  3259,  1257,  8675,\n",
       "          8635, 12953, 11658,  6728,  8616,  1268,  1561,  1375,  2171,    44,\n",
       "             0,  5293],\n",
       "        [ 2623,  1554, 12953,  3304,  8056,  2181, 11593,  2172,  7343,  4482,\n",
       "            44,     0,  1265,  8011,  1633,  7958,  3608,  2153,  1330, 12953,\n",
       "          1927, 10405,  2168,  8490,  9121,  2181,  2067,    44,     0,  5015,\n",
       "         11090,  3871],\n",
       "        [10723,  1341,  2840,  2041,  8678, 12953,  4790,  1779,  1529,  7860,\n",
       "         10385,  7365,  9286,    44,     0,  3360, 11593,  2900,  2592,  2896,\n",
       "          6078,  4867, 12953,  8750,  1487,  5123,  1771, 11167,  1487,  5774,\n",
       "            44,     0],\n",
       "        [ 8966, 10385,  1915, 10793,  2057, 11022,  1375, 12953, 10437,  2176,\n",
       "          1375, 11605,  1337,  4798,  2848,    44,     0,  3695,  7094, 11959,\n",
       "          1310,  1267, 11959,  3256, 12953,  3867,  3867, 11572, 11034, 10541,\n",
       "          1375,  4814],\n",
       "        [ 9123,  9214,  4899,  2527,  2812,  6258,  6487,    44,     0,  5691,\n",
       "          2176,  7826, 12016,  4093, 12772, 10175, 12953,  5017,  8446,  9370,\n",
       "          4854,  4489,  3703,  6611,    44,     0,  3699, 10995,  7521, 11435,\n",
       "          3306,  1804],\n",
       "        [ 1308,  8598,    44,     0,     0,  5244,  1879, 12718,  5532,  1486,\n",
       "          4318,  5163, 12953,  9098,  3783,  8491,  9369,  5745,  1398,  2192,\n",
       "            44,     0,  5073,  4799,  3732,  3380,  4851, 11572,  6680, 12953,\n",
       "          1330,  2241],\n",
       "        [11658,  7921,  3233,  5049, 12953,  1299, 10202,  2901,  2651,  1885,\n",
       "            44,     0,  9910,  9721,  4093,  4899,  7826, 12953,  5234, 10202,\n",
       "          9370,  6848,  9649,    44,     0,  2041, 11992,  2615,  7984,  2141,\n",
       "         12953,  3826],\n",
       "        [ 6763,  5025,  2181,    44,     0, 10142, 12016,  6048,  6048,  6223,\n",
       "          3868,  9968, 12953,  2901,  7125,  2068,  1367, 11835,  8653,  3894,\n",
       "            44,     0,  1346, 10187,  4895,  3370,  3663,  6258,  6258, 12953,\n",
       "          2156,  7535],\n",
       "        [11761, 10970,  9957, 10356,    44,     0,  6728,  2101,  4851,  2922,\n",
       "          4075,  5619, 11245, 12953, 10531,  5019, 11735,  2794, 11421,  3380,\n",
       "          4867,    44,     0,  1880,  3241,  1257, 11421,  1867,  5047,  3882,\n",
       "         12953,  5123],\n",
       "        [ 6078,  2880,    44,     0,  4277, 11206, 11956,  7400, 12953,  2077,\n",
       "          1274,  3835,  3259,  2113,    44,     0,  1672,  1672,  4851, 10303,\n",
       "          4752, 10793,  1474,    44,     0,  1257,  2901, 12016, 11811,  4894,\n",
       "          1755,  2681],\n",
       "        [ 9751,  6854,  2754,  2911,  2895, 10879, 11167, 12953,  5070,  3259,\n",
       "          2613,  1291,  5088,  7386,  9098,    44,     0,  5010, 11634,  9100,\n",
       "          5470, 10984, 11714, 10159, 12953,  9119,  5106,  8675,  7994,  7373,\n",
       "          1359,  1956]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9393,    44,     0,  1666,  8986,  4756, 12410,  8558, 12953, 11718,\n",
       "          6188,  5512, 11399,  5465,    44,     0,  6445,  6487,  5047,  8135,\n",
       "          6428, 12953,  4822,  5024,  1257, 11416,  4808,    44,     0,     0,\n",
       "          6087,  5870],\n",
       "        [   44,     0,  4620, 12110,  6728,  5619,  2208,  1267,  2087, 12953,\n",
       "          9283,  4093, 12016, 11759,  2966,  4822, 11984,    44,     0,  2033,\n",
       "          1342, 12807,  3303,  8966, 11083,  6531, 12953,  9233,  8829,  7826,\n",
       "          9048, 11481],\n",
       "        [11165,  5736,  7373, 10988,    44,     0,  8985,  2911, 11250,  6032,\n",
       "          1845,  1885,  6223, 12953,  7420,  3719, 12718,  1346, 11703,  5602,\n",
       "          7925,    44,     0, 12113,  1267,  4819, 12953,  2241, 11735,  7922,\n",
       "            44,     0],\n",
       "        [ 2087,  9903,  9752, 12953,  1276,  1264,  9214,  5045,  3783,  3368,\n",
       "          9418,    44,     0, 10595,  3960,  4889, 11612,  5663,  3645,  1952,\n",
       "         12953,  2968,  7768,  1387, 10159,  6489,  2901,  3607,    44,     0,\n",
       "          4851,  4782],\n",
       "        [12953,  1375,  1375,  4412,  4318,  2146,  2921,  8459,    44,     0,\n",
       "          4347,  5244,  5602, 10602, 10602,  1267,  3882, 12953,  4564,  8011,\n",
       "          6602,  6487, 11572,  2442,  4026,    44,     0,  8422,  8422,  1831,\n",
       "         11811,  3737],\n",
       "        [ 3663,  8598,  5085, 12953,  1770,  2033,  3696,  5088,  2122,  1342,\n",
       "          3696,    44,     0,  9010,  9025,  1299,  3259,  1257,  8675,  8635,\n",
       "         12953, 11658,  6728,  8616,  1268,  1561,  1375,  2171,    44,     0,\n",
       "          5293,  2605],\n",
       "        [ 1554, 12953,  3304,  8056,  2181, 11593,  2172,  7343,  4482,    44,\n",
       "             0,  1265,  8011,  1633,  7958,  3608,  2153,  1330, 12953,  1927,\n",
       "         10405,  2168,  8490,  9121,  2181,  2067,    44,     0,  5015, 11090,\n",
       "          3871,  5592],\n",
       "        [ 1341,  2840,  2041,  8678, 12953,  4790,  1779,  1529,  7860, 10385,\n",
       "          7365,  9286,    44,     0,  3360, 11593,  2900,  2592,  2896,  6078,\n",
       "          4867, 12953,  8750,  1487,  5123,  1771, 11167,  1487,  5774,    44,\n",
       "             0, 12330],\n",
       "        [10385,  1915, 10793,  2057, 11022,  1375, 12953, 10437,  2176,  1375,\n",
       "         11605,  1337,  4798,  2848,    44,     0,  3695,  7094, 11959,  1310,\n",
       "          1267, 11959,  3256, 12953,  3867,  3867, 11572, 11034, 10541,  1375,\n",
       "          4814,    44],\n",
       "        [ 9214,  4899,  2527,  2812,  6258,  6487,    44,     0,  5691,  2176,\n",
       "          7826, 12016,  4093, 12772, 10175, 12953,  5017,  8446,  9370,  4854,\n",
       "          4489,  3703,  6611,    44,     0,  3699, 10995,  7521, 11435,  3306,\n",
       "          1804,  1644],\n",
       "        [ 8598,    44,     0,     0,  5244,  1879, 12718,  5532,  1486,  4318,\n",
       "          5163, 12953,  9098,  3783,  8491,  9369,  5745,  1398,  2192,    44,\n",
       "             0,  5073,  4799,  3732,  3380,  4851, 11572,  6680, 12953,  1330,\n",
       "          2241,  1327],\n",
       "        [ 7921,  3233,  5049, 12953,  1299, 10202,  2901,  2651,  1885,    44,\n",
       "             0,  9910,  9721,  4093,  4899,  7826, 12953,  5234, 10202,  9370,\n",
       "          6848,  9649,    44,     0,  2041, 11992,  2615,  7984,  2141, 12953,\n",
       "          3826, 10339],\n",
       "        [ 5025,  2181,    44,     0, 10142, 12016,  6048,  6048,  6223,  3868,\n",
       "          9968, 12953,  2901,  7125,  2068,  1367, 11835,  8653,  3894,    44,\n",
       "             0,  1346, 10187,  4895,  3370,  3663,  6258,  6258, 12953,  2156,\n",
       "          7535, 10912],\n",
       "        [10970,  9957, 10356,    44,     0,  6728,  2101,  4851,  2922,  4075,\n",
       "          5619, 11245, 12953, 10531,  5019, 11735,  2794, 11421,  3380,  4867,\n",
       "            44,     0,  1880,  3241,  1257, 11421,  1867,  5047,  3882, 12953,\n",
       "          5123,  3233],\n",
       "        [ 2880,    44,     0,  4277, 11206, 11956,  7400, 12953,  2077,  1274,\n",
       "          3835,  3259,  2113,    44,     0,  1672,  1672,  4851, 10303,  4752,\n",
       "         10793,  1474,    44,     0,  1257,  2901, 12016, 11811,  4894,  1755,\n",
       "          2681,    44],\n",
       "        [ 6854,  2754,  2911,  2895, 10879, 11167, 12953,  5070,  3259,  2613,\n",
       "          1291,  5088,  7386,  9098,    44,     0,  5010, 11634,  9100,  5470,\n",
       "         10984, 11714, 10159, 12953,  9119,  5106,  8675,  7994,  7373,  1359,\n",
       "          1956,    44]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "bias = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 1152])\n",
      "Split the last dimension into q, k, v\n",
      "Shape of q, k, v: torch.Size([16, 32, 384]), torch.Size([16, 32, 384]), torch.Size([16, 32, 384])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(batch_size, block_size, n_embd)\n",
    "attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "print(attn(x1).shape)\n",
    "print(f'Split the last dimension into q, k, v')\n",
    "q, k, v = torch.split(attn(x1), split_size_or_sections=n_embd, dim=2)\n",
    "print(f'Shape of q, k, v: {q.shape}, {k.shape}, {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of q, k, v: torch.Size([16, 6, 32, 64]), torch.Size([16, 6, 32, 64]), torch.Size([16, 6, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "# calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "q = q.view(batch_size, block_size, n_head, n_embd // n_head).transpose(1, 2)\n",
    "k = k.view(batch_size, block_size, n_head, n_embd // n_head).transpose(1, 2)\n",
    "v = v.view(batch_size, block_size, n_head, n_embd // n_head).transpose(1, 2)\n",
    "print(f'Shape of q, k, v: {q.shape}, {k.shape}, {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "mask = torch.tril(torch.ones(block_size, block_size))\n",
    "mask = mask.view(1, 1, block_size, block_size) # Add the batch dimension\n",
    "mask[:, :, :block_size, :block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_proj = nn.Linear(n_embd, n_embd, bias)\n",
    "attn_dropout = nn.Dropout(dropout)\n",
    "resid_dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "att = att.masked_fill(mask=(mask[:, :, :block_size, :block_size] == 0), value=float('-inf'))\n",
    "att = F.softmax(att, dim=-1)\n",
    "att = attn_dropout(att)\n",
    "y1 = att @ v # (B, nh, T, T) x (B, nh, T, hs) => (B, nh, T, hs)\n",
    "# (B, nh, T, hs) => (B, T, nh, hs) => (B, T, nh * hs) => (B, T, C)\n",
    "y1 = y1.transpose(1, 2).contiguous().view(batch_size, block_size, n_embd)\n",
    "y1 = c_proj(y1)\n",
    "y1 = resid_dropout(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"The remainder of embedding and head number should be zero.\"\n",
    "        \n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer('bias', torch.tril(torch.ones(block_size, block_size))\n",
    "                                          .view(1, 1, block_size, block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch_size, sequence length (block_size), embedding dimension (n_embd)\n",
    "        \n",
    "        q, k, v = torch.split(attn(x), split_size_or_sections=n_embd, dim=2)\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(mask=(self.bias[:, :, :block_size, :block_size] == 0), value=float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        return resid_dropout(c_proj(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dim, bias) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(n_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_dim)) if bias else None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(n_embd, 4 * n_embd, bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_embd, bias)\n",
    "        self.attn = SelfAttention()\n",
    "        self.ln_2 = LayerNorm(n_embd, bias)\n",
    "        self.mlp = MLP()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 32\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 384 \n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50304, 384]), torch.Size([50304, 384])\n"
     ]
    }
   ],
   "source": [
    "emb_w = nn.Embedding(config.vocab_size, config.n_embd).weight\n",
    "proj_w = nn.Linear(config.n_embd, config.vocab_size, bias=False).weight\n",
    "print(f'{emb_w.shape}, {proj_w.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "drop = nn.Dropout(config.dropout)\n",
    "self_attn = nn.ModuleList([SelfAttentionBlock() for _ in range(config.n_layer)])\n",
    "ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "\n",
    "lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=config.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, t = x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = wte(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.arange(0, t, dtype=torch.long)\n",
    "pos_emb = wpe(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = drop(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in self_attn:\n",
    "    h = block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ln_f(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0030, -0.5735, -0.4376,  ...,  0.9856,  0.5710,  0.4209],\n",
       "         [ 0.6976, -1.5383, -0.1592,  ...,  1.1898, -0.7068,  0.2978],\n",
       "         [-0.7431, -0.7057,  0.2619,  ...,  0.2048, -0.3446,  1.0029],\n",
       "         ...,\n",
       "         [ 0.6525, -0.0219,  0.2504,  ..., -0.7797,  0.7056,  1.6049],\n",
       "         [ 0.4847,  0.5181,  0.0750,  ..., -0.4665, -1.7071,  1.8080],\n",
       "         [ 1.0355,  1.2601, -0.5118,  ..., -0.3321,  0.2384, -0.3286]],\n",
       "\n",
       "        [[ 0.2986,  2.1307,  0.2097,  ...,  0.3658,  0.9099,  0.6240],\n",
       "         [ 0.6353,  0.2701,  0.6960,  ...,  0.3434,  0.3542,  0.7578],\n",
       "         [-0.6637,  0.8668,  1.2509,  ..., -0.6774,  0.9275,  1.9576],\n",
       "         ...,\n",
       "         [ 0.4944,  0.2033,  0.6882,  ..., -1.0972,  2.5491,  0.6758],\n",
       "         [ 1.2667,  0.2470, -0.3598,  ..., -1.3504,  2.0244,  0.8600],\n",
       "         [ 0.5276,  1.5340, -0.3205,  ..., -0.8318,  1.2741,  0.3036]],\n",
       "\n",
       "        [[-0.1550, -0.3403,  0.4283,  ..., -0.7204,  0.3657,  1.2489],\n",
       "         [ 0.9159, -0.4431,  0.4249,  ..., -0.5023,  0.3518,  0.1112],\n",
       "         [-0.2964, -0.0259, -0.1941,  ..., -1.8969,  0.6207,  0.6881],\n",
       "         ...,\n",
       "         [ 0.3755, -0.8890,  0.7879,  ..., -0.5798,  1.6522,  0.9009],\n",
       "         [ 0.6394,  0.3358,  0.2009,  ..., -0.8590,  1.4877,  0.9737],\n",
       "         [ 0.6789,  0.1217,  0.1553,  ...,  0.3002,  1.3988,  0.1774]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.9903, -0.6238, -1.3220,  ...,  0.8675, -0.5960,  0.5413],\n",
       "         [ 1.2807, -1.0496, -0.1061,  ..., -0.4284, -1.8155,  0.9274],\n",
       "         [-0.1813, -0.2856, -0.3490,  ..., -1.0012, -0.7976,  1.4896],\n",
       "         ...,\n",
       "         [-0.3846, -0.6605, -0.2837,  ...,  0.5073, -0.8435,  1.6057],\n",
       "         [ 1.1133, -0.2476,  0.0137,  ..., -0.7173, -0.5665,  1.5893],\n",
       "         [ 0.6509,  0.1219, -0.7971,  ..., -0.1599, -2.0748,  0.4633]],\n",
       "\n",
       "        [[ 0.7978, -0.8019, -0.5347,  ..., -0.3556, -0.0133, -0.4824],\n",
       "         [ 1.1132, -1.4675, -0.2521,  ..., -0.8409,  0.1239, -0.0865],\n",
       "         [-0.5019,  0.1239, -0.5377,  ..., -0.9029,  1.3758, -0.3114],\n",
       "         ...,\n",
       "         [-0.7339, -0.7971,  0.9936,  ..., -1.3199,  0.7433,  0.4126],\n",
       "         [-0.0678,  0.3588,  0.7963,  ..., -1.4487,  0.6804,  1.5169],\n",
       "         [-0.5654,  1.6882, -0.6192,  ..., -1.0988,  1.5115,  0.4794]],\n",
       "\n",
       "        [[-1.2234, -0.9303, -1.0631,  ..., -0.6001,  0.4051,  0.2856],\n",
       "         [ 0.3273, -0.7757, -1.0272,  ..., -1.4820, -0.7939, -0.2524],\n",
       "         [-1.4391,  0.2910, -0.8456,  ..., -2.5255,  1.1388,  0.3157],\n",
       "         ...,\n",
       "         [ 0.0130, -1.4199,  0.3296,  ..., -1.2532,  1.1535,  0.3342],\n",
       "         [ 0.0319,  1.0424,  0.2461,  ..., -1.4451,  1.2720,  0.9141],\n",
       "         [-0.2409, -0.0036, -0.0899,  ..., -1.2744,  0.3212,  0.4342]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 384])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 384])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_step = res[:, [-1], :] # note: using list [-1] to preserve the time dim\n",
    "last_step.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 50304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0696, -0.2707, -0.2855,  ..., -0.0197,  0.2865,  0.3227]],\n",
       "\n",
       "        [[-0.3631, -0.3368,  0.5571,  ...,  0.9414,  0.3481,  0.8444]],\n",
       "\n",
       "        [[-0.6672, -1.0062,  0.0357,  ...,  0.4258,  0.2164, -0.3530]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4421, -0.4289, -0.6132,  ...,  0.7328,  0.0855,  1.0328]],\n",
       "\n",
       "        [[-0.1924, -0.5834, -0.0645,  ...,  1.2259,  0.2577,  0.6021]],\n",
       "\n",
       "        [[-0.4020, -0.8917,  0.5011,  ..., -0.0258,  0.3603,  0.7311]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_logits = lm_head(last_step)\n",
    "print(inference_logits.shape)\n",
    "inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0696, -0.2707, -0.2855,  ..., -0.0197,  0.2865,  0.3227],\n",
       "        [-0.3631, -0.3368,  0.5571,  ...,  0.9414,  0.3481,  0.8444],\n",
       "        [-0.6672, -1.0062,  0.0357,  ...,  0.4258,  0.2164, -0.3530],\n",
       "        ...,\n",
       "        [-0.4421, -0.4289, -0.6132,  ...,  0.7328,  0.0855,  1.0328],\n",
       "        [-0.1924, -0.5834, -0.0645,  ...,  1.2259,  0.2577,  0.6021],\n",
       "        [-0.4020, -0.8917,  0.5011,  ..., -0.0258,  0.3603,  0.7311]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inference_logits[:, -1, :].shape)\n",
    "inference_logits = inference_logits[:, -1, :]\n",
    "inference_logits / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3181],\n",
       "        [24246],\n",
       "        [30504],\n",
       "        [23911],\n",
       "        [ 6131],\n",
       "        [47783],\n",
       "        [28942],\n",
       "        [18260],\n",
       "        [43682],\n",
       "        [14037],\n",
       "        [ 2503],\n",
       "        [26882],\n",
       "        [22515],\n",
       "        [12443],\n",
       "        [38892],\n",
       "        [34382]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(inference_logits, dim=-1)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm_head(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 50304])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7096,  0.0828, -0.3921,  ...,  0.1466, -0.0737, -0.3202],\n",
       "        [ 0.3355,  0.1282,  0.3670,  ...,  0.0767,  0.3652, -0.6138],\n",
       "        [ 0.1141,  1.1284,  0.5404,  ..., -0.1263, -0.0282, -0.6873],\n",
       "        ...,\n",
       "        [ 0.4092, -0.5623,  0.2820,  ..., -0.8355,  0.2703, -0.1455],\n",
       "        [-0.0415, -0.6487,  0.3678,  ...,  0.1901, -0.3452, -0.1307],\n",
       "        [-0.4020, -0.8917,  0.5011,  ..., -0.0258,  0.3603,  0.7311]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits.view(-1, logits.size(-1)) # (B, T, C) => (B * T, C)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12016,  1328,  1561,  3306,  4851,  2171,    44,     0,  3696,  3696,\n",
       "          4505, 12682, 11070, 11701,  5695, 12953,  1387,  3696,  3882, 12682,\n",
       "          3240,  4851,  1846,    44,     0, 12682,  2656,  2039, 10019,  5944,\n",
       "          5734,  3783],\n",
       "        [ 5024, 12953, 11206, 11213,  1480,  2880,  5881,  5847,  3621,    44,\n",
       "             0,     0,  9140, 11832,  2901,  1264,  7762,  1398,  3222, 12953,\n",
       "         11956,  4905,  1375, 11605,  3306,  7373,  3259,    44,     0,  4122,\n",
       "          3882,  3836],\n",
       "        [ 2020,  6763,  4822,  2184,  1268, 10571,  2262,    44,     0,  2033,\n",
       "          3696, 11513,  4301,  1299,  1480,  1341, 12953,  1257,  3234,  4851,\n",
       "         10520,  7922,  3715, 12089,    44,     0,  2954, 12016,  1676, 10970,\n",
       "         10321,  5619],\n",
       "        [12953,  3225,  3905, 11713,  2880,  4905,  1578,  9957,    44,     0,\n",
       "          3316,  3140,  5047,  1894,  4851,  1299,  2651, 12953, 11194,  6647,\n",
       "          5039, 11409,  4980,  7696,  2873,    44,     0,     0,  4899,  5974,\n",
       "          2049,  5088],\n",
       "        [ 9046,  9128,  2428,  1567,  1781, 12953,  8231,  7356,  4826,  2150,\n",
       "          5040,    44,     0,  4106,  1780,  3312,  2872,  1952, 12953,  5630,\n",
       "          3238,  8966, 11881,  5844,    44,     0,  1257,  1257,  1957,  1265,\n",
       "          3206, 12953],\n",
       "        [ 5106,  6723,  2311, 12953, 12113, 11480,  3245,  8966, 12124,    44,\n",
       "             0,  3204,  1346,  1480,  2880,  3264, 12953,  5244,  1264,  4554,\n",
       "          9286,  8446,    44,     0,     0,  8450,  3348, 11826,  3370,  1265,\n",
       "         12953,  3703],\n",
       "        [12953,  1994,  3797,  4899, 12334,  9370,  2946,  2839,    44,     0,\n",
       "             0,  1267,  3206,  3300,  7399,  1267,  2672,  7789, 12953,  1312,\n",
       "          9600,  7298,  3649,  1274, 11713,  8515,    44,     0,  5025,  4867,\n",
       "         10539, 11167],\n",
       "        [ 9123, 12953,  4404,  5077,  7927,  9098,  2880,  2880,  3300,    44,\n",
       "             0,     0,  5240,  5209,  7856,  5235, 10937,  5946,  5814, 12953,\n",
       "          2249,  1747,  4581,  7994,  4383,  5606,  4808,    44,     0,  7826,\n",
       "          5088,  4425],\n",
       "        [ 2047,    44,     0,  4239,  4035,  4899,  1755,  1257,  2039,  5653,\n",
       "            44,     0,  8286,  3977, 11598, 11593,  4553,  2071,  2113, 12953,\n",
       "          3204,  7400,    44,     0,  6763, 10311,  1281, 12016,  8966,  1601,\n",
       "          5208,    44],\n",
       "        [    0,     0,  5762, 10165,  2172,  3370,  2624, 12953, 11737,  9233,\n",
       "          2901,  1265,  2921,    44,     0,  2901,  2199,  1303,  9470,  3403,\n",
       "         12953,  1333,  4564,  5825,  5984,  6127,    44,     0,  3595,  3413,\n",
       "          8675,  9741],\n",
       "        [ 6263, 11761,  5976,  8670, 12953,  3597,  3597,  2151,  5681,  2086,\n",
       "          1346,  5231,    44,     0,  2901, 10947,  2809,  3857,  5978,  5773,\n",
       "          5974, 12953, 12016, 10970, 11409,  2873,  9370,  5984, 11593,    44,\n",
       "             0,  8675],\n",
       "        [10159,  9418,    44,     0,  2603,  3714,  5025,  5944,  4888, 12953,\n",
       "          2812,  8678,  4838,  2885, 10988,    44,     0,  4482,  9968,  1857,\n",
       "          6078, 11611, 12953,  6646,  1369,  8605,  1375, 10794,    44,     0,\n",
       "          3276,  6614],\n",
       "        [ 4851, 11682, 12953,  4765,  2623,  3695,  3344,  5025,  4307,  3969,\n",
       "            44,     0, 10317,  3882, 11572,  3225, 10942,  5602,  8008, 12953,\n",
       "          5012,  3306,  3811,  8100, 11206,  1281,  8149,    44,     0,     0,\n",
       "          7994, 11245],\n",
       "        [ 3864,  2896,  2873,  1267,  1638, 12953,  1887,  3306, 10793,  4318,\n",
       "          1268,  2901,  1340,    44,     0,  2901,  1769,  1379, 11606, 10146,\n",
       "          9119,  7094, 12953,  7299,  7762,  2158,  2250,  2234,  1768,  8413,\n",
       "            44,     0],\n",
       "        [ 7535, 10400, 12768,  3871,  1502,  3240, 12953,  3381,  1469,  5745,\n",
       "          3857, 10234,  5512,  4808,    44,     0, 11956,  7535,  2900,  4320,\n",
       "          1487, 11707,  5205, 12953,  3381,  1469,  2140,  3694,  5337,  9660,\n",
       "          9723,    44],\n",
       "        [   44,     0,  1257,  5050, 11962,  4884, 10348, 12953,  2034, 11245,\n",
       "          3268,  1273,  2744,    44,     0,  3656,  4826,  6109, 12953, 10923,\n",
       "          4384, 10291, 12953,  2812, 12028,  3315,    44,     0,  8908,  1291,\n",
       "          4790,  1261]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1).shape # (B, T, 1) => (B * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9734, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, y.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = nn.ModuleDict(dict(\n",
    "    wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "    wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "    drop = nn.Dropout(config.dropout),\n",
    "    self_attn_block = nn.ModuleList([SelfAttentionBlock() for _ in range(config.n_layer)]),\n",
    "    ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "))\n",
    "\n",
    "lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=config.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = transformer.wte(x)\n",
    "device = x.device\n",
    "b, t = x.size()\n",
    "pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "pos_emb = transformer.wpe(pos)\n",
    "h = transformer.drop(tok_emb + pos_emb)\n",
    "for block in transformer.self_attn_block:\n",
    "    h = block(h)\n",
    "res = transformer.ln_f(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9427, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the loss\n",
    "logits = lm_head(res)\n",
    "F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 50304])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for generation in inference\n",
    "logits = lm_head(res[:, [-1], :])\n",
    "logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
